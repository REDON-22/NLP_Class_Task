{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4bf56c",
   "metadata": {},
   "source": [
    "# NLP QUIZ # 01:\n",
    "\n",
    "**GROUP MEMBERS:**\n",
    "\n",
    "- MUHAMMAD SHAFIQ ASHFAQ (CSC-20F-118)\n",
    "- SYED MUHAMMAD AALIYAN (CSC-20F-171)\n",
    "- OWAIS AZAD (CSC-20F-139)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c282d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85%; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85%; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c395adf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "import numpy\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a131fa",
   "metadata": {},
   "source": [
    "### 1. Load data given in quiz folder using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76cd0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_data = pd.read_csv('Dataset/imdb_urdu_reviews_train.csv', usecols=[0], header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3f59c",
   "metadata": {},
   "source": [
    "### 2. Identify shape of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf561042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40001, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b01c63e",
   "metadata": {},
   "source": [
    "### 3. Show complete description of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3628d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>39738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>آج کا شو پسند آیا !!! یہ ایک قسم تھی اور نہ صر...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "count                                               40001\n",
       "unique                                              39738\n",
       "top     آج کا شو پسند آیا !!! یہ ایک قسم تھی اور نہ صر...\n",
       "freq                                                    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde1bc9",
   "metadata": {},
   "source": [
    "### 4. Assign column name of text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd0b701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0                                             review\n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...\n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...\n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...\n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.rename(columns={0:'Text'}, inplace=True)\n",
    "# text_data = text_data.drop(0)\n",
    "# text_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b81834",
   "metadata": {},
   "source": [
    "### 5. Apply word and sentence tokenizer and print value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5ef0ef",
   "metadata": {},
   "source": [
    "#### Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110a61b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review',\n",
       " 'میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائیڈ (اسکائینجر ہنٹ پہلو نے مجھ سے اپیل کی تھی) کی علامت پر مبنی ایک نوعمر کی حیثیت سے ٹیپ کیا ، اس فلم کی کوئی معلومات یا توقعات نہیں تھیں۔ کتنی خوشگوار حیرت ہے جب میں نے اسے دیکھا! یہ ایک ایسی تفریحی فلم تھی اور مجھے یاد ہے کہ اسے بار بار دیکھا جاتا تھا۔ میں نے سوچا کہ یہ تصور اچھی طرح سے نافذ کیا گیا ہے ، میں نے مختلف گروہوں کے مابین بے ضرر مسابقت کا لطف اٹھایا ، اور مجھے لگتا ہے کہ مقتول کا شکار خود ہی کافی ہوشیار تھا۔ کبھی کبھی ایسا لگتا ہے کہ لوگوں کو فلموں سے بہت زیادہ توقعات وابستہ ہیں۔ تمام فلموں میں ایک بھاری \"پیغام\" یا شاندار اداکاری ، پروڈکشن کی قدریں ، یا خصوصی اثرات مرتب نہیں ہوں گے۔ بعض اوقات فلمیں صرف تفریح \\u200b\\u200bاور تفریح \\u200b\\u200bکے لئے ہوتی ہیں اور یہ دونوں سطحوں پر کامیاب ہوتی ہے۔ جڑواں اداکار اداکاروں کے تبصرے پڑھ کر بہت اچھا لگا۔ میں نے یہ فلم برسوں میں نہیں دیکھی ، لیکن اگر میں یہ سمجھتا تھا کہ میں اس کے بارے میں اتنا ہی گرم اور جوش و خروش سے اپنا ردعمل ظاہر کروں گا جیسے میں نے نوعمری کی طرح کیا تھا۔ یہاں تک کہ جب میں یہ ٹائپ کرتا ہوں ، پنیر ابھی تک موزوں تھیم گانا کے ٹکڑوں کو اپنے سر سے چلا رہے ہیں: \"جب آدھی رات کا جنون آپ کو ملنا شروع ہوجاتا ہے ... اس سے کوئی فرق نہیں پڑتا ہے کہ آپ کیا کہتے ہیں ، اس سے کوئی فرق نہیں پڑتا ہے کہ آپ کیا کرتے ہیں۔ ..! \"',\n",
       " 'چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹون کو پسند کیا تھا ، لہذا میں اس فلم کو دیکھنے گیا۔ میں نے اپنا پیسہ ضائع کیا۔ پلاٹ بہت پتلا تھا۔ نیز ، فلم مجھے زیادہ دن دلچسپی نہیں بنا سکی۔ مجھے خوشی ہوئی کہ یہ ختم ہوچکا ہے۔ اگر آپ انسپکٹر گیجٹ دیکھنا چاہتے ہیں تو ، اس کے بجائے کارٹون دیکھیں۔ یہ فلم سے کہیں بہتر تھا۔',\n",
       " 'ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ہے جو ایک ایسے والد کا پیدا کرتا ہے جیسے البرٹ ٹی فٹزجیرالڈ ، جو ہم پہلے طیارے میں ملتے ہیں ، جب وہ اس جگہ کی طرف جارہا تھا جہاں اس نے بہت عرصہ پہلے ترک کردیا تھا ، اور جہاں اس نے اپنی بیوی اور ایک بچ leftہ چھوڑا تھا۔ اب اس پر ایک ذہنی طور پر معذور لڑکے کے قتل کا الزام ہے۔ جب ہم اسے پہلی بار دیکھتے ہیں ، اس نے اس اخبار میں ایک عنوان پکڑا ہے جس کے سامنے والی عورت پڑھ رہی ہے۔ بلکہ بے رحمی کے ساتھ ، وہ اس سے پوچھتی ہے کہ کیا وہ اخبار رکھ سکتا ہے ، اور وہ خاتون دوسرے حصے پیش کرتی ہے۔ ٹھیک ہے ، یہ وہ نہیں ہے جو اس نے پوچھا ، وہ عورت کیا کرنا چاہتی ہے ، اسے وہ سامنے والا حص giveہ دے رہا ہے جس کو وہ پڑھ رہی ہے۔ میتھیو ریان ہوج نے اس پریشان کن فلم کو لکھا اور ہدایت کی ہے جو ہمارے معاشرے کی طرح کئی طرح سے عکاسی کرتی ہے۔ در حقیقت ، مسٹر ہوج بالکل اسی طرف اشارہ کر رہے ہیں جو اس میں غلط ہے۔ اس فلم میں لیلینڈ کو ایک نوعمر عمر پیش کیا گیا ہے ، جو خیالی اور حقیقت میں بھی فرق نہیں کرسکتا ہے۔ یہ بات عیاں ہے کہ کسی انسان کو قتل کرنا ، یہاں تک کہ اس پیارے اور معصوم لڑکے کو بھی جس نے اس کے مستحق ہونے کے لئے کچھ نہیں کیا ، اس کے نہ صرف اپنے لئے ، بلکہ اپنے خاندان اور مقتول لڑکے کے کنبہ کے لئے بھی مہلک نتائج برآمد ہوں گے۔ حقیقت میں ، ایسا لگتا ہے کہ لیلینڈ کو اس کے بارے میں کچھ پتہ نہیں ہے کہ وہ اس جرم کے ارتکاب کے لئے کس چیز کی ترغیب دیتا ہے کیونکہ وہ اس کے بارے میں توبہ نہیں کرتا ہے۔ ظاہر ہے کہ اس کے والدین کی طلاق سے لیلینڈ کو صدمہ پہنچا ہے۔ اس کا اپنا باپ ایک متلو .ن آدمی ہے جو اس کی پرواہ نہیں کرتا تھا۔ یہ پرل ، نوعمر حراستی مرکز میں استاد ہے جو نوجوان کے اندر ہنگامہ دیکھتا ہے اور مدد کرنا چاہتا ہے ، لیکن بدقسمتی سے ، اس کے پاس کوئی موقع نہیں ہے۔ فلم میں سب سے اچھی بات ڈان چیڈل ہے ، جو ایک عمدہ اداکار ہے جو ہمیشہ بچاتا ہے۔ . مسٹر ہوج کی ہدایت کاری کے تحت یہ جوڑا کاسٹ اچھ workا کام کرتا ہے۔ کیون اسپیس کے پاس ملزم قاتل کے مغرور والد کا کردار ادا کرنے کا ایک اچھا موقع ہے اور ہم اسے تکبر کرنے والے بیوقوف ہونے کی وجہ سے نفرت کرتے ہیں۔ اگرچہ تھوڑی دیر تک ، فلم ہمیں اس کے جوابات سے کہیں زیادہ سوالات دیتی ہے۔',\n",
       " 'مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری میں گریڈ زیڈ جلدی۔ ٹم تھامرسن کا 13 انچ کا کلینٹ ایسٹ ووڈ جیسا کاپ بیرونی خلا سے ایک بدصورت اڑنے والا سر (!) زمین کا پیچھا کرتا ہے اور جنوبی برونکس میں ایک گینگ وار میں شامل ہو جاتا ہے! رحمدلی سے مختصر ، لیکن مہلک طور پر کم ، اس کے 50 ایفٹ وومن کے حملے کے بعد کے سب سے پُر اثر اثرات ہیں۔ انہیں بھی تسلسل والے آدمی کو برطرف کرنا چاہئے تھا: نوٹ کریں کہ تھومرسن کی دھوپ ہر دوسرے شاٹ میں کیسے غائب ہوتی ہے اور دوبارہ ظاہر ہوتی ہے۔ ہنسی مذاق کی بات ہے ، لیکن ہم کیوں یہ فلمیں دیکھتے ہیں ، کیوں نہیں؟ سیکول oll ڈول مین بمقابلہ. شیطانی کھلونے (مبینہ طور پر اس سے بھی بدتر ہیں ، اگر یہ ممکن ہو تو.0 (**** کا)',\n",
       " 'یہ کولمبو ہے جس کی ہدایتکاری اپنے کیریئر کے ابتدائی وقت پر اسٹیون اسپیلبرگ نے کی تھی۔ یہ سنسنی خیز کچھ نہیں ہے لیکن اس فلم میں اسپلبرگ کے لئے آنے والی عظیم چیزوں کے کچھ چھوٹے اشارے دیکھے جاسکتے ہیں۔ فلم بنیادی طور پر اسی انداز میں ہے جیسے اسپیئل برگ کی زیادہ تر 70 کی فلموں اور ٹی وی کے کام۔ تو اس کا مطلب یہ ہے کہ کچھ کردار کچھ عجلت کو ظاہر کرتے ہیں اور نہیں میں صرف کولمبو کردار کے بارے میں بات نہیں کر رہا ہوں۔ نرالا پن کی قسم جو شاید 1975 کی اسپلبرگ فلم \"جب\" میں بہترین طور پر دیکھی جاسکتی ہے۔ لیکن عام ابتدائی اسپیلبرگ عناصر کے کچھ چھوٹے اشارے کے علاوہ ، آپ اس فلم کو بڑھتے ہوئے ہدایتکار اسٹار کی کارگر اور عمدہ مثال کے طور پر نہیں کہہ سکتے ہیں۔ یہ بری بات نہیں ، یقینا it یہ نہیں ہے جیسا کہ میں نے پہلے کہا تھا ، یہ بھی کوئی سنسنی خیز بات نہیں ہے۔ یہ فلم واقعتا well اچھ wellی اور بہت ہی امید افزاء آغاز ہوئی تھی لیکن ٹھیک ٹھیک کھلنے کے بعد ، جس میں ہمیشہ کی طرح قتل ہوا ، فلم زیادہ سست اور دیکھنے کے لئے بھی مدھم ہو گیا۔ دھندلا کیونکہ اس کتاب کی زیادہ تر ایک کولمبو فلم ہے جس میں حقیقی یادگار لمحات نہیں ہوتے ہیں ، پھیکا نہیں ہوتا ہے کیونکہ یہ دیکھنا ایک بورنگ فلم ہے۔ قتل خود ہی کافی ذہین تھا اور جرم لکھنے والے اپنے قص partnerے کے ساتھی کو قتل کرنے کا تصور تھا۔ کچھ عمدہ اور دلچسپ صلاحیت کا مظاہرہ کیا۔ تاہم کہانی نے واقعتا اپنے تمام امکانات کی کھوج نہیں کی۔ کم از کم یہی احساس ہے کہ اس فلم نے مجھے چھوڑ دیا ہے۔ اس کے باوجود جیک کیسڈی کے کردار کا شکریہ ادا کرنے کے لئے یہ فلم ابھی بھی اچھی تھی ، جو سمجھتا ہے کہ وہ اس کے ذہین / جرم لکھنے کے تجربے کی وجہ سے کولمبو کا ہوشیار ہے اور اسے سب کچھ دینے کی کوشش کرتا ہے خود سے دور ہونے والے ممکنہ اشارے کی قسمیں۔ لیکن یقینا کولمبو بہتر جانتا ہے اور وہ پہلے ہی لمحے سے اس کا پہلا نمبر ملزم ہے لیکن وہ ہمیشہ کی طرح اس کھیل کو بھی کھیلتا ہے۔ مووی کا مجموعی انداز اچھا ہے اور اس میں کیمرہ کی کچھ اچھی پوزیشن اور ترمیم کا استعمال کیا گیا ہے۔ یہ دیکھ کر حیرت کی بات ہے کہ اس میں سے زیادہ تر اسپیلبرگ کے بعد کے کام ، خاص طور پر کچھ کیمرہ زاویوں کے ساتھ مطابقت رکھتا تھا۔ ایک عمدہ اور کامل طور پر قابل نظارہ کولمبو مووی لیکن اس کے ساتھ منسلک اسپلبرگ کا نام بھی آپ کی توقعات کو بڑھا مت دو۔ انتہائی 7/10',\n",
       " 'مجھے اس فلم کا بیشتر حصہ پسند آیا۔ جیسا کہ دوسرے جائزوں نے بتایا ہے کہ اس میں اچھی کاسٹ ہے ، پلاٹ کافی دلچسپ ہے۔ یہ سب دیکھنا ہی اچھا ہے۔ لیکن مجھے لگتا ہے کہ ، اختتام مکمل طور پر دب گیا ہے ، اس نے مجھے حیرت میں مبتلا کردیا۔ ہاں ، آپ توقع کرتے ہیں کہ اس طرح کی مووی میں لوگ ایک دوسرے کو عبور اور ڈبل کراس کرتے ہیں ، لیکن چوگنی حد عبور کرتے ہیں؟ ٹھیک ہے ، اگر یہ سازش کے ذریعہ جائز ہے تو پھر کیوں نہیں؟ لیکن یہ بری بات ہے ، اس کی مکمل ضرورت نہیں ہے۔ ایک خاص نقطہ کے بعد یہ سب کچھ مکمل طور پر کوئی معنی نہیں رکھتا ہے۔ (یہاں اسپیکر آتا ہے)۔ ہوائی اڈ scene منظر کے بعد اینریکو اور اس کے ساتھیوں کے پاس پہلے ہی رقم ہے۔ میں باقی اسکینڈل کی ضرورت کو نہیں سمجھ سکا۔ کیا فیڈریکو کی ناک کو صرف اس حقیقت میں رگڑنا ضروری ہے کہ اسے بے وقوف بنایا گیا ہے؟ میں اسے نہیں خریدتا ہوں۔ لہذا فلم کے 3/4 کے لئے 10 میں سے 6 اور اختتام پر 10 میں سے 2۔',\n",
       " 'ٹھیک ہے ، شاید یہ آسکر کا مستحق نہیں ہے۔ یا گولڈن گلوب۔ یا کوئی ایوارڈ ، اس معاملے کے لئے۔ اداکاری غیر معمولی ہے ، ہدایت نامہ کو کریڈٹ دینے کی کوئی وجہ نہیں ہے ، اور یہ واقعی 21 ویں صدی کی ایک اور نیم گوری فلم ہے جس میں زیادہ تر لوگ مہذب خیال کریں گے۔ یا شاید خوفناک بھی۔ لیکن میری رائے میں ، اس سب سے تھوڑا سا فرق نہیں پڑتا ہے۔ اور یہی وجہ ہے کہ مجھے اس فلم کو دیکھنے میں بہت اچھا لگا۔ یقین ہے ، پہلے 40 منٹ بہت ہی آہستہ ہیں ، لیکن جیسے جیسے فلم آگے بڑھ رہی ہے ، اس میں آپ کو کچھ پسند آئے گا ، اگر آپ کی طرح ہو جیسے میں ہوتا ، اور کسی اور کو بھی یہ دیکھنا چاہئے۔ فلم اور وہ یہ ہے: 2 گھنٹے تفریح \\u200b\\u200b، بے محل تشدد کی تلاش۔ (اور کک گدا ختم ہونے والا ، جس کو میں خراب نہیں کروں گا۔) ہاں ، اس مووی میں بہت ساری خامیاں ہیں ، لیکن سامنے والے سرورق پر موجود کاسٹ لسٹ آپ کو بیوقوف بنانے نہ دیں۔ ہلٹن نے ایک اچھی کارکردگی پیش کی جو کسی نے آتے ہی نہیں دیکھا۔ یہاں تک کہ اس کے سب سے بڑے نفرت کرنے والے مجھ اور میرے دوستوں کو بھی اس بات پر متفق ہونا پڑا کہ اس نے اپنی مشکل سے قابل اعتماد اداکاری کی مہارت اور ایک ایسی پٹی چھیڑ چھاڑ سے ہمیں بہت حیران کردیا جو آن لائن پاپ اپ اس سے وعدہ کرنے والی کسی بھی چیز کی طرح ناگوار نہیں تھا ، لیکن پھر بھی ایسا نہیں ناقابل برداشت کے طور پر تیزی سے آگے بڑھانا یا DVD بند کرنا۔ اس میں تشدد نے مجھے حیران کردیا۔ اسکول میں یا ہارر بورڈ میں کوئی بھی اس کے بارے میں بات نہیں کر رہا تھا جیسے وہ \"سو \\'اور\\' ہوسٹل \\'نامی یہ نئی فلمیں تھیں لیکن میں محفوظ طور پر یہ کہہ سکتا تھا کہ\" موم \"سو سے زیادہ گرافک تھا ، اور کچھ موت کے مناظر دراصل تھے کافی پریشان کن۔ آخر میں ، مجھے حیرت نہیں ہے ہاؤس آف موم نے آئی ایم ڈی بی ٹاپ 250 میں جگہ نہیں بنائی ، لیکن یہ یقینی طور پر دیکھنے کے قابل ہے۔',\n",
       " 'میں نے اسے سائنس فائی چینل پر دیکھا۔ یہ پہلے والے کے بعد دائیں طرف آیا۔ کسی وجہ سے اس فلم نے میری دلچسپی برقرار رکھی۔ مجھے نہیں معلوم کیوں ، پوچھنا چھوڑ دیں ۔--- اسپیولرز --- ٹھیک ہے ... یہ حیرت انگیز تھا کہ یہ لڑکا فلم بنانے میں کس طرح شامل ہوا۔ پہلی فلم میں ، اس کے پاس لوگوں کو مارنے کی ایک \"وجہ\" تھی ، لیکن اس سلسلے میں ، آدھے قتل / قتل کی بنیادی وجہ بلا وجہ تھی۔ اسٹینلے نے تخلیقی اختلافات کی وجہ سے ہدایت کار کو مار ڈالا ، انہوں نے تخلیقی اختلافات کے سبب شریک مصنف کو پکڑ لیا ، لیکن کاسٹ کو مارنے کی کوشش کرنے سے کیا معاملہ ہوا؟ کوئی کاسٹ ، کوئی فلم نہیں۔ وہ چاہتا تھا کہ \"جب وہ مرے تو حقیقی دکھائی دیں\"؟ اگر یہ اتنی زیادہ بجٹ والی فلم بننے والی تھی تو ، خصوصی اثرات کا استعمال کریں ، MAN۔ بالکل پہلی ہی کی طرح ، پکڑی گئی لڑکی بھاگ گئی ، اور اسٹینلے گڑبڑ میں مبتلا ہوکر ختم ہوگئی۔ ووو (طنز) اس فلم میں صلاحیت موجود ہے۔ اور سب کی سب سے افسوسناک چیز ... واقعی افسوسناک حصہ ... میں ایک \"کیبن بذریعہ جھیل 3\" دیکھتا۔ صرف اس وجہ سے کہ میں جڈ نیلسن کو پسند کرتا ہوں ، اور اس سلسلے کے بارے میں وہ واحد اچھا حصہ ہے۔',\n",
       " 'یہ فلم ایک ناقص مووی تھی۔ پلاٹ خراب تھا اور کامیڈی جس کی انہوں نے \"کوشش\" کی تھی وہ خراب طور پر سامنے آئی۔ ایسا لگتا ہے کہ حادثات متناسب اور متوقع ہیں۔ میں نے سوچا کہ اداکاروں نے کچھ حد تک کوشش کی لیکن اس فلم کے ساتھ ، یہ اتنا لنگڑا تھا کہ صرف اتنی دور تک جاسکتی ہے۔ ایک بدترین فلموں میں سے جو میں نے دیکھی ہے اور کسی کو بھی اس کی سفارش نہیں کرتے ہیں۔ مسٹر ایکسیڈنٹ کا واحد حادثہ تھا اس کی رہائی۔',\n",
       " 'یہ ایم جی ایم اور فرینک سناتراس بدترین فلموں میں سے ایک ہونی چاہئے۔ ایک اوڈ بال میوزیکل کامیڈی جو تقریبا ہر پہلو میں ناکام ہوجاتی ہے۔ سلیٹ پلاٹ میں سیناترا ایک چومنگ بینڈ کے طور پر اپنے باپ کی ساکھ کو جاری رکھنے کی کوشش کر رہا ہے۔ وہ کوئی ڈاکو نہیں ہے اور بوسہ نہیں لیتا ہے !! وہ \"اعصابی\" کردار نبھاتا ہے نیز اس مکالمے کی وجہ سے بھی اس کی توقع کی جا سکتی ہے۔ سین چوری کرنے والے جے کرول ناش اور ملڈرڈ نیٹ ورک ہیں۔ بہت بری بات ہے کہ ان کے ساتھ مل کر مزید مناظر نہیں تھے۔ میں نے فلم کو دو ستارے دیئے ہیں کیونکہ سیٹ اور ملبوسات بہتر ہیں اور کتھرین گریسن \"محبت جہاں آپ ڈھونڈیں گے\" نے گایا ہوا ایک گانا سنسنی خیز ہے۔ ہوسکتا تھا کہ اس کی بحالی ہوسکتی ہے۔ نیز ، مزاح نگار قسم کا ایک مزاحیہ قسم کا رقص جو بذریعہ RIDCARDO Manttalban، CYD CHARISSE اور ANN MILER اگر مذاق ہے۔ تو صرف ان وجوہات اور ان وجوہات کی بناء پر ، یہ قابل دید ہے۔ بوسہ بانٹ فرینک سیناترا ابتدائی سالوں کے جمع کرنے کا ایک حصہ ہے۔',\n",
       " \"آپ کو ٹی وی مووی یا سیریز کے پائلٹ کے لئے بنائے گئے اس چیز کو دیکھنے میں زیادہ وقت خرچ کرنے کی ضرورت نہیں ہے یا جو کچھ بھی اس کا ارادہ کیا گیا ہے اس کا پتہ لگانا ہے کہ اسٹور میں کیا ہے۔ ناقابل یقین حد تک خراب میوزیکل سکور شروع سے ہی اپنی شروعات کرتا ہے۔ سنجیدگی سے ، اگر یہ بدترین تھیم میں نے کبھی نہیں سنا ہے تو ، میں یقینی طور پر اسے یاد نہیں کرسکتا ہوں۔ اگرچہ اداکاری کا ہنر یہاں موجود ہے ، جیف برجس سے لے کر کارل بیٹز ، ویرا میلز اور سال مینو تک ، تحریر انتہائی اشتعال انگیز ہے اور اس کی کہانی کو متنازعہ ، دقیانوسی دقیانوسی تصورات سے بھرا ہوا ہے ، اور کین کیسی کا ایک واضح رسپ۔ ہالی ووڈ کو ہمیشہ ساٹھ کی دہائی کی کہانیاں کیوں پیش کرنا چاہ؟ گو کہ نام نہاد ہیپی سب ایک ہی جہتی مورگن ہیں؟ یہ بہت خراب ہے کہ ہمارے غیر معمولی معاشرتی تجربے میں اس طرح کے ایک دلچسپ دور کو عام طور پر باہر اور باہر کوڑے دان کے ذریعہ دکھایا گیا ہے تاکہ اس نوع کی کم سے کم جارحیت کو اب معقول طور پر قبول کیا جائے جب اس میں سے کوئی بھی چیزیں واقعتا really اس انداز کے قریب نہیں آتی ہیں۔ . میں نے آج تک دیکھا سب سے بہتر ایک یادداشت ہے جسے بیکر نامی لڑکے نے 'لوک بیک بیک' کہا ہے ، لیکن اس کے بارے میں اور کس نے سنا ہے؟ ہالی ووڈ میں کوئی نہیں ، یہ یقینی طور پر ہے۔ وہ حقیقت میں پریشان ہونے کے لئے کسی فلم کے اس کرونر کی طرح ٹریپ کو آگے بڑھانے میں بہت مصروف ہیں۔\",\n",
       " 'ایرک بوگوسین کے اسٹیج پلے کا ایک بہت ہی عمدہ ہدایت والا ورژن۔ بوگوسیئن کے عمدہ کرداروں کے لئے اور جو بھی یہ دیکھنا چاہتا ہے کہ فلموں میں کسی ڈرامے کو صحیح طریقے سے کیسے لایا جائے اس کے بارے میں جانچ پڑتال کے قابل ہے۔',\n",
       " 'یہ فلم بنیادی طور پر ایک روسی طوائف کے بہن / دوست کے جنازے کے لئے اپنے آبائی گاؤں واپس آنے کی کہانی ہے۔ یہاں ایک اور چھوٹی سی کہانی کی معمولی لائنیں ہیں جو واقعی میں لینے سے کہیں زیادہ دلچسپ ہوسکتی ہیں ، لیکن ان کی پوری تلاش نہیں کی جاتی ہے۔ اس فلم کی اصل بات جنازے ، اٹھنا اور بعد میں کرون کی ایک برادری کے مستقبل پر ہونے والے تنازعہ ہے جو گڑیا بناتی ہیں اور انہیں ووڈکا خریدنے کے لئے بیچ دیتی ہیں لیکن اب وہ فنکار لاپتہ ہیں جنہوں نے اپنی گڑیا کو منڈی بنا دیا۔ بظاہر ، فلم غیر شائستہ ہے۔ شہر سے لے کر گائوں تک طوائف کا سفر ایک حیرت انگیز حد تک نہ ختم ہونے والی ٹرین کی سواری اور کیچڑ سے ہوتا ہے۔ ہوسکتا ہے کہ اس نے ہمیں روسی زمین کی تزئین کی بہتات سے متاثر کیا ہو۔ گاؤں میں ، جیسے کہ یہ ہے ، بیوائوں اور ایک مرد کی لشکر کے ذریعہ آباد ہے ، جو مردہ بچی کا ساتھی ہے۔ گڑیا کے کاروبار کو جاری رکھنا ہر ایک کے ل for پریشانی کا باعث ہوتا ہے اور آخر کار ناممکن لگتا ہے۔ زیادہ تر فلم کی شوٹنگ ایک ہاتھ سے پکڑے ہوئے کیمرہ سے کی گئی ہے جو متلی کو متاثر کرسکتی ہے۔ مغربی ناظرین کے لئے ایک اور مسئلہ یہ ہے کہ سب ٹائٹلز میں کرونوں کے گانا اور نوحہ شامل نہیں ہیں۔ اس فلم میں مت جائیں جب تک کہ آپ روسی زبان میں روانی نہیں رکھتے ہیں۔',\n",
       " 'اس فلم کے بارے میں مجھے صرف ایک ہی چیز کہنے کی ضرورت ہے۔ وہ منظر جس میں شق میوزیکل نمبر میں ہے جس میں فرانسس کیپرا کے کردار کے ساتھ جنن بننا چاہتا ہے۔ یہ فلم کبھی نہ دیکھیں۔ کہانی خوفناک ہے ، اداکاری بھیانک ہے (کام ، یہ شق ہے!) اور میں اس فلم کو دیکھنے سے پہلے ہی فری ولی (اتنا ہی خوفناک) میں کیپرا کے بجائے دو بار دیکھوں گا۔',\n",
       " 'فلم کے آغاز سے ہی یہ احساس ملتا ہے کہ ہدایتکار کچھ پیش کرنے کی کوشش کر رہا ہے ، میرا کیا کہنا ہے کہ اس کہانی کے بجائے جس انداز میں فلم بنانی چاہئے ، وہ اس کے برعکس چلا گیا ، ایک قسم کی چال ہے جو وہ بنانا چاہتا ہے ، اور اس کے سوٹ کرنے کے لئے ایک کہانی لکھی۔ اور وہ اس میں بہت بری طرح سے ناکام رہا ہے۔ مجھے لگتا ہے کہ وہ ایک سجیلا فلم بنانے کی کوشش کر رہا تھا۔ کسی بھی طرح سے مجھے لگتا ہے کہ یہ فلم وقت اور کوششوں کا کل ضائع ہے۔ ہدایتکار کے کریڈٹ میں ، وہ میڈیا کو جانتے ہیں جس کے ساتھ وہ کام کر رہے ہیں ، میں جو کہنے کی کوشش کر رہا ہوں وہ یہ ہے کہ میں نے اس سے بدتر فلمیں دیکھی ہیں۔ یہاں کم از کم ہدایتکار فلم میں تسلسل برقرار رکھنا جانتے ہیں۔ اور اداکاروں نے بھی عمدہ پرفارمنس دی ہے۔',\n",
       " 'یہ ایک فلم ہے جس کے بارے میں ایک کالے آدمی نے ایک ایئر لائن کمپنی خریدی ہے اور اس کمپنی کو اوپر کے طیارے میں افریقی مرکز میں تبدیل کیا ہے۔ یہاں تک کہ وہ مالک کو پیش کرتے ہیں کہ نہ صرف ایئر لائن کے کنٹرول میں ہیں ، بلکہ ہوائی اڈے پر ایئر ٹرمینل کے کچھ حصے کو بھی کنٹرول کرتے ہیں۔ ایک دن جب یہ شخص اگلی بار آپ کو دیکھے گا تو وہ 100 ملین ڈالر جیت جاتا ہے ، وہ ہوائی اڈے کے مالک کی طرح کام کرتے ہوئے پورے ہوائی اڈے پر چل رہا ہے۔ ہر ایک اس مووی کو ایک پیروڈی کہتے ہیں ، لیکن اس مووی کے بارے میں کچھ بھی نہیں محو! یہ فلم فلاپ ہے اور وال مارٹ میں 95 4.95 کے لئے ہمیشہ کے لئے رہے گی۔ میں یہاں تک اس معاملے میں بھی نہیں آسکتا کہ ایم جی ایم اس فلم پر 16 ملین ڈالر کیوں ضائع کرے گی۔ یہ فلم براہ راست سے ویڈیو کی حیثیت کی بھی ضمانت نہیں دیتی ہے۔ لکھنے والوں (ایک سیاہ اور ایک سفید) کو ہالی ووڈ سے ہمیشہ کے لئے بلیک بل کیا جائے۔ نہ صرف وہ سیاہ فاموں کو زیادہ دقیانوسی تصور کرتے ہیں بلکہ انہیں جاہل انسان کے طور پر پیش کرتے ہیں۔ مجھے ایسی فلم میں جانے سے شرم آتی ہے جو مجھے مسلسل ذلیل کرتی ہے۔ جب ویڈیو ٹی بی ایس پر دکھائی جاتی ہے تو ، ویڈیو اسٹور ، وال مارٹ ، ہر منظر کے مطابق ، یا اتوار کی دوپہر کو اپنا وقت ضائع نہ کریں۔',\n",
       " 'دور دراز کے جزیرے کے چڑیا گھر میں بجلی کی بندش کے سلسلے کے بعد ، جینیاتی طور پر انجنیئر سابرٹوتھ ٹائیگر جزیرے کے ڈھیلے اور نوکیلے رہائشیوں پر ہیں۔ یار ، سائنس فائی چینل نے کچھ خراب \"اصل\" فلمیں بنائیں ، لیکن مجھے لگتا ہے کہ شاید یہ اب تک کی ان کی بدترین ہو! اس بری طرح سے لکھے ہوئے اور ہدایت کردہ \"جوراسک پارک\" کے چھاپوں نے معمول کی تمام پیش کشوں کو پیش کیا ہے (جن پاگل سائنس دانوں کا خیال ہے کہ راکشسوں کے ہاتھوں مارے گئے لوگ \"قابل برداشت نقصانات\" ہیں ، اکیلے لمبے اندھیرے دالانوں میں چلنے والے کردار ، دماغی مردہ نوعمر کردار جن میں صرف کام ہوتا ہے) فلم ایک خوفناک موت ، وغیرہ کی موت ہے) ، اور ، حیرت کی بات نہیں ، اس میں کوئی شک نہیں ہے۔ اس کے خاص اثرات ناگوار ہیں - عنوان کی خوبصورتی کے قریب قریب کٹھ پتلی سر کافی خراب ہیں ، جیسے آلیشان گڑیا کی طرح نظر آرہے ہیں ، لیکن سی جی آئی — جس نے کمپیوٹر سے تیار کردہ ڈایناسور کو \"ڈایناسور کے ساتھ چلنا\" میں تقابل کے ذریعہ زندگی جیسی نظر آتی ہے۔ صرف سیدھے سراسر خوفناک۔ یہاں کچھ گور ہے ، لیکن اس میں سے بیشتر خوبصورت غیر یقینی دکھائی دیتے ہیں۔ اوہ ، اور آخر میں سائنس دان کی موت کا منظر واقعتا the ایک بدترین چیزوں میں سے ایک ہے جو میں نے برسوں میں دیکھا ہے۔ میں ہنس بھی نہیں سکتا تھا کہ یہ بہت برا تھا! اپنا وقت ضائع نہ کرو ، یہ بالکل سیدھا برا ہے ۔2 / 10۔ اوہ ، اور آپ کے لئے یہاں کچھ دلچسپ باتیں ہیں — اس فلم نے 2003 کی سائنس فائی فلم \"ایلین ہنٹر\" سے موسیقی کے اشارے لئے تھے ، جو اس گھٹیا پن سے کہیں بہتر تھا۔',\n",
       " 'سبھی سازشی تھرلرز میں سے ایک بہترین اور دلچسپ ، (1970 کی دہائی میں ایسی متعدد فلمیں تھیں)۔ یہ ایک نیوکلئیر ری ایکٹر میں پیش آنے والے ایک حادثے اور اس کو چھپانے کی کوششوں کے بارے میں ہے۔ جیک لیمون وہ ملازم ہیں جو یہ سمجھتے ہیں کہ پلانٹ کتنا خطرناک ہے اور جین فونڈا صلیبی ٹیلی ویژن کے رپورٹر ہیں جو کہانی کو بورڈ پر لیتے ہیں اور دونوں کھلاڑی ان کی بہترین کارکردگی کا مظاہرہ کرتے ہیں۔ فلم میں پیش کیے گئے واقعات یقینا ter خوفناک ہیں اور ان میں نہیں کم از کم دور کی بات ، (یہاں واقعی زیادہ تر تھری مایل جزیرے میں ہوا ہے) ، اور اگرچہ اب یہ کچھ دور کی طرح لگتا ہے ، اس کے باوجود ، اس بات پر روشنی ڈالتا ہے کہ ہم نے اپنے لئے جو دنیا بنائی ہے وہ کتنا نازک اور خطرناک ہے۔ . جلد ہی کسی بھی وقت گلوبل وارمنگ کے اثرات کے بارے میں متعدد ایسی ہی فلموں کی توقع کریں ، (اور \"کل کے بعد کے دن\" کی طرح کوڑے دان نہ بنائیں)۔',\n",
       " 'جان لیگوزامو ایک بہترین مزاح نگار اور کہانی سنانے والا ہے۔ ہر بار جب یہ HBO پر ہوتا ہے تو مجھے اسے روکنا پڑتا ہے۔ جان یہ کہانی سناتا ہے کہ وہ کیسے بڑا ہوا (شاید کچھ حقیقت اور افسانہ) اور درمیان میں مزاحیہ کہانیاں شامل کرتا ہے۔ اگر آپ جان کی کامیڈی پسند کرتے ہیں تو مجھے کہنا پڑے گا یہ ان کی بہترین کامیڈی ہے۔']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenized = text_data[\"Text\"].to_list()\n",
    "sentence_tokenized[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c418fd",
   "metadata": {},
   "source": [
    "#### Word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c0f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customWordTokenizer(sentence):\n",
    "    tokens = sentence.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2610691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "      <td>[review]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "      <td>[میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "      <td>[چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "      <td>[ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "      <td>[مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             review   \n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...   \n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...   \n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...   \n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...   \n",
       "\n",
       "                                              tokens  \n",
       "0                                           [review]  \n",
       "1  [میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...  \n",
       "2  [چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...  \n",
       "3  [ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...  \n",
       "4  [مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data['tokens'] = text_data[\"Text\"].apply(customWordTokenizer)\n",
    "text_data[['Text','tokens']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a47c88",
   "metadata": {},
   "source": [
    "### 6. Apply lemmitization and steaming on given text and print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefa73a",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9668ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stemFunction(token_list):\n",
    "    txt_list = []\n",
    "    for i in token_list:\n",
    "        txt = stemmer1.stem(i)\n",
    "        txt_list.append(txt)\n",
    "    txt_list = \" \".join(txt_list)\n",
    "    return txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07f1481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "      <td>[review]</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "      <td>[میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...</td>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "      <td>[چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...</td>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "      <td>[ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...</td>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "      <td>[مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...</td>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             review   \n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...   \n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...   \n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...   \n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                           [review]   \n",
       "1  [میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...   \n",
       "2  [چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...   \n",
       "3  [ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...   \n",
       "4  [مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...   \n",
       "\n",
       "                                               stemm  \n",
       "0                                             review  \n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  \n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  \n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  \n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer1 = PorterStemmer()\n",
    "text_data['stemm'] = text_data[\"tokens\"].apply(stemFunction)\n",
    "text_data[['Text','tokens','stemm']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebf9a7",
   "metadata": {},
   "source": [
    "##### lemmitization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b97254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaFunction(token_list):\n",
    "    txt_list = []\n",
    "    for i in token_list:\n",
    "        txt = lemmatizer1.lemmatize(i)\n",
    "        txt_list.append(txt.lower())\n",
    "    txt_list = \" \".join(txt_list)\n",
    "    return txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aedad1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review</td>\n",
       "      <td>[review]</td>\n",
       "      <td>review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "      <td>[میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...</td>\n",
       "      <td>میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "      <td>[چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...</td>\n",
       "      <td>چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "      <td>[ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...</td>\n",
       "      <td>ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "      <td>[مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...</td>\n",
       "      <td>مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0                                             review   \n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...   \n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...   \n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...   \n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0                                           [review]   \n",
       "1  [میں, نے, اسے, 80, کی, دہائی, کے, وسط, میں, ای...   \n",
       "2  [چونکہ, میں, نے, 80, کی, دہائی, میں, انسپکٹر, ...   \n",
       "3  [ایک, ایسے, معاشرے, کی, حالت, کے, بارے, میں, ت...   \n",
       "4  [مفید, البرٹ, پیون, کی, طرف, سے, ایک, اور, ردی...   \n",
       "\n",
       "                                               lemma  \n",
       "0                                             review  \n",
       "1  میں نے اسے 80 کی دہائی کے وسط میں ایک کیبل گائ...  \n",
       "2  چونکہ میں نے 80 کی دہائی میں انسپکٹر گیجٹ کارٹ...  \n",
       "3  ایک ایسے معاشرے کی حالت کے بارے میں تعجب کرتا ...  \n",
       "4  مفید البرٹ پیون کی طرف سے ایک اور ردی کی ٹوکری...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer1 = WordNetLemmatizer()\n",
    "text_data['lemma'] = text_data[\"tokens\"].apply(lemmaFunction)\n",
    "text_data[['Text','tokens','lemma']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ab2b4",
   "metadata": {},
   "source": [
    "### 7. Pass text from stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db0f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')  # You need to download the stopwords for the first time\n",
    "\n",
    "\n",
    "def stopwords_remover(sentence, stopW):\n",
    "    words = sentence.split()\n",
    "    rr = []\n",
    "    for word in words:\n",
    "        if word not in (stopW):\n",
    "            rr.append(word)\n",
    "        r = re.sub(\"[^a-z #@]\", \"\", \" \".join(rr))\n",
    "        input_txt = re.sub(\"@[\\w]*\", \"\", r)\n",
    "    \n",
    "    \n",
    "    return input_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15009f4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\urdu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\HP\\Desktop\\NLP_Quiz_1\\Quiz_1 copy.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/NLP_Quiz_1/Quiz_1%20copy.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39;49mwords(\u001b[39m'\u001b[39;49m\u001b[39murdu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/NLP_Quiz_1/Quiz_1%20copy.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m text_data[\u001b[39m'\u001b[39m\u001b[39mnltk_stop_word_removed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m text_data[\u001b[39m'\u001b[39m\u001b[39mlemma\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(stopwords_remover, stopW\u001b[39m=\u001b[39mstop_words)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/HP/Desktop/NLP_Quiz_1/Quiz_1%20copy.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m text_data[[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlemma\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnltk_stop_word_removed\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwords\u001b[39m(\u001b[39mself\u001b[39m, fileids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, ignore_lines_startswith\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m line_tokenize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[39m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopen(f) \u001b[39mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[39m.\u001b[39mappend(fp\u001b[39m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root\u001b[39m.\u001b[39;49mjoin(file)\u001b[39m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(\u001b[39mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decorator\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[39m=\u001b[39m (args[\u001b[39m0\u001b[39m], add_py3_data(args[\u001b[39m1\u001b[39m])) \u001b[39m+\u001b[39m args[\u001b[39m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m init_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\python 3.10\\lib\\site-packages\\nltk\\data.py:312\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m _path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(_path)\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(_path):\n\u001b[1;32m--> 312\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo such file or directory: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path \u001b[39m=\u001b[39m _path\n",
      "\u001b[1;31mOSError\u001b[0m: No such file or directory: 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\urdu'"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('urdu'))\n",
    "text_data['nltk_stop_word_removed'] = text_data['lemma'].apply(stopwords_remover, stopW=stop_words)\n",
    "text_data[['Text','lemma', 'nltk_stop_word_removed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9553460",
   "metadata": {},
   "source": [
    "### 8. Create a list of other stoptwords append with English stopwords in NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8894f0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stops_words = ['.', 'ha', 'ja', 'aa', 'rr']\n",
    "# custom_stops_words.extend(stop_words)\n",
    "# print(custom_stops_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec368d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ثٌبًب', 'ثبری', 'ًکتہ', 'ثراں', 'ثبلا', 'عووهی', 'کرے', 'کورے', 'خوًہی', 'ضکٌب', 'ہو', 'ارد', 'صفر', 'ثلٌذ', 'کیوں', 'خوکہ', 'دور', 'آئی', 'اضطرذ', 'دیکھیں', 'اہن', 'کر', 'علاوٍ', 'رریعے', 'هکول', 'اخبزت', 'پوچھو', 'دکھبتب', 'ہوبری', 'خبهوظ', 'اضکے', 'کل', 'کے', 'ڈھوًڈلیب', 'اش', 'آخر', 'چکی', 'ایک', 'آئے', 'ضکتب', 'اختتبم', 'ہوئی', 'خبًب', 'کبم', 'گیب', 'رکھ', 'اپٌے', 'صورتوں', 'دیتی', 'لڑکپي', 'لوجے', 'کوتر', 'پچھلا', 'ضیکٌڈ', 'صورتیں', 'هگر', 'تبزٍ', 'هسطوش', 'زبل', 'طرف', 'پر', 'ضبلوں', 'ثرآں', 'لوگ', 'آیب', 'چبہٌب', 'گب', 'غروعبت', 'ثٌذکرو', 'دیتے', 'هٌبضت', 'دی', 'پیع', 'تو', 'کہے', 'ہوا', 'چکب', 'ضکتے', 'کورٍ', 'ًکبلٌب', 'پورا', 'رکھب', 'ثیچ', 'رہے', 'ثبلترتیت', 'ًبپطٌذ', 'کہیں', 'خیطب', 'اپٌب', 'گئی', 'کھولٌب', 'ضوچتی', 'زصہ', 'دیکھو', 'لی', 'ًئے', 'چھوٹوں', 'تریي', 'ثدبئے', 'هیں', 'کت', 'کہوں', 'پوچھتب', 'ثہتری', 'ثہتر', 'راضتہ', 'دکھبئیں', 'کہب', 'آج', 'ضوچتب', 'پہلےضی', 'ہورہب', 'طور', 'پہلی', 'گروپ', 'یہبں', 'در', 'ثہت', 'غبیذ', 'لئے', 'ثھرپور', 'ڈھوًڈیں', 'اٹھبًب', 'کرًب', 'ہیں', 'دکھبتی', 'ترتیت', 'کرا', 'خص', 'ہوچکب', 'لیٌب', 'ارکبى', 'چبہب', 'اوًچبئی', 'ہی', 'اچھے', 'فی', 'کوًطی', 'ثلکہ', 'ضکے', 'خبًتب', 'کررہی', 'رکھتی', 'ہوتی', 'هطئلہ', 'چکیں', 'گروٍ', 'کیوًکہ', 'زبلات', 'ہورہی', 'دلچطپ', 'گٌتی', 'ایطے', 'دوًوں', 'ثڑی', 'اکیلے', 'ہن', 'آدهی', 'تھے', 'ًقطہ', 'هتعلق', 'ثب', 'ثڑا', 'خلذی', 'لگیں', 'کوئی', 'لوجی', 'علاقے', 'پبش', 'کی', 'ہر', 'ثٌذی', 'چھوٹی', 'کہو', 'هعلوم', 'هطلق', 'دورى', 'ڈھوًڈی', 'ختن', 'آٹھ', 'هیری', 'هیرے', 'ادھر', 'پبًچ', 'اکٹھے', 'کرتب', 'ثغیر', 'کھولو', 'چلو', 'تر', 'یہ', 'هڑے', 'هیرا', 'ضت', 'زقبئق', 'ضے', 'تھوڑی', 'زبضر', 'پھر', 'ثبعث', 'ثہتریي', 'ضوچ', 'ثعذ', 'اضتعوبل', 'اکٹھب', 'خواى', 'قطن', 'فرد', 'چبہے', 'هسترهہ', 'آًب', 'کرتی', 'گرد', 'خو', 'کہتے', 'دلچطپی', 'ثٌبرہی', 'هوکٌبت', 'تھی', 'ضوچیں', 'ًطجت', 'علاقہ', 'رہب', 'عذد', 'خجکہ', 'والے', 'ہوں', 'ڈھوًڈو', 'قجیلہ', 'تن', 'ضکب', 'ثھرا', 'خگہ', 'دیر', 'وار', 'هوکي', 'ضوچی', 'زکن', 'ضوچو', 'ًبگسیر', 'توہیں', 'دیکھی', 'دکھبًب', 'ہوتے', 'طریقہ', 'کن', 'پوچھوں', 'ًہیں', 'کہ', 'اردگرد', 'ضبل', 'اکٹھی', 'زصے', 'گی', 'کئی', 'هڑًب', 'چلے', 'هڑا', 'رکھی', 'چھوٹے', 'زقیقت', 'صورت', 'رہی', 'پل', 'زبصل', 'هسترم', 'دوضری', 'ثرش', 'درخبت', 'زیبدٍ', 'هػتول', 'ہوچکے', 'هطتعول', 'لو', 'ثبرے', 'کہتی', 'گروہوں', 'کہتب', 'ہوضکتے', 'ضبرا', 'ًیب', 'پہلا', 'ڈھوًڈا', 'ضیذھی', 'ضبرے', 'ہوًب', 'ہوضکتی', 'کہٌب', 'تھب', 'لیں', 'لگتب', 'آش', 'ضبت', 'عظین', 'ہوبرا', 'دیتب', 'طریقے', 'رکي', 'هسیذ', 'کوًطے', 'دفعہ', 'اضکی', 'ہوئے', 'اکیلی', 'رریعہ', 'قجل', 'لازهی', 'لگتی', 'ضکی', 'تھوڑے', 'پوچھب', 'دیب', 'ثٌذ', 'ضیذھے', 'دو', 'خب', 'کیطے', 'ضلطلہ', 'خگہوں', 'طریقوں', 'کئے', 'غے', 'لوسبت', 'اًذر', 'خبًٌب', 'ٹھیک', 'ثھر', 'توبم', 'هلا', 'هہرثبى', 'کجھی', 'ضوچب', 'اور', 'چکے', 'ثڑے', 'زصوں', 'پبئے', 'تب', 'صسیر', 'طریق', 'اضکب', 'خبر', 'هوکٌہ', 'زکویہ', 'ہوضکتب', 'دلچطپیبں', 'دوضرے', 'لوگوں', 'ہوچکی', 'پراًب', 'ہے', 'زبلیہ', 'لوجب', 'پوچھٌب', 'علاقوں', 'خبًتے', 'ضوچٌب', 'گے', 'لگی', 'اوًچے', 'درزقیقت', 'یقیٌی', 'ڈھوًڈًب', 'درخے', 'لیب', 'چھوٹب', 'درضت', 'کرتے', 'کھولیں', 'هطئلے', 'رکھتے', 'صبف', 'کیب', 'راضتے', 'لوسہ', 'ضرور', 'دوضرا', 'ضکتی', 'کرو', 'چھہ', 'راضتوں', 'اضتعوبلات', 'خگہیں', 'تھوڑا', 'دیکھٌب', 'ظبہر', 'زقیتیں', 'اگرچہ', 'ثٌبرہب', 'اچھب', 'خیطبکہ', 'ثٌبرہے', 'کھولا', 'کریں', 'طورپر', 'لگتے', 'درخہ', 'چیسیں', 'کیے', 'دش', 'ثٌذکرًب', 'ضوچتے', 'هطبئل', 'ًئی', 'کہی', 'کوًطب', 'پہلے', 'تٌہب', 'ضیذھب', 'دکھبو', 'کبفی', 'غذ', 'کھولے', 'اچھی', 'تت', 'اغیب', 'ہورہے', 'تک', 'اطراف', 'طب', 'لے', 'کرتبہوں', 'گئے', 'تیي', 'غخص', 'اکثر', 'صورتسبل', 'پوچھتے', 'پوچھیں', 'دیٌب', 'افراد', 'ثٌب', 'اوًچب', 'اکیلا', 'ہوگئی', 'ضرورت', 'چبر', 'خٌبة', 'رکھے', 'ہوبرے', 'دوضروں', 'هختلف', 'غروع', 'ثھی', 'پوچھتی', 'دے', 'چلیں', 'ہوًی', 'پبًب', 'اى', 'کورا', 'دکھبتے', 'ضبدٍ', 'خططرذ', 'الگ', 'کب', 'خبًتی', 'ضروری', 'اوًچی', 'ہوگیب', 'لگے', 'ہوًے', 'کوروں', 'ًوخواى', 'کوى', 'کوطي', 'تعذاد', 'چلا', 'ثبہر'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "nlp_urdu = spacy.blank('ur')\n",
    "custom_stops_words = nlp_urdu.Defaults.stop_words\n",
    "print(custom_stops_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b3a397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data['custom_stop_word'] = text_data['lemma'].apply(stopwords_remover,stopW=custom_stops_words)\n",
    "text_data[['Text','lemma', 'custom_stop_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d7f7fc",
   "metadata": {},
   "source": [
    "### 9. Add other stopwords in notepad file and append with previous and pass text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8599ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "notepad_stops_words = ['lol', 'a', 'kk']\n",
    "# notepad_stops_words.extend(custom_stops_words)\n",
    "with open(r'notepad_stops_words.txt', 'w') as fp:\n",
    "    for item in notepad_stops_words:\n",
    "        fp.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47df5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "notepad_stops_words = []\n",
    "\n",
    "# open file and read the content in a list\n",
    "with open(r'notepad_stops_words.txt', 'r') as fp:\n",
    "    for line in fp:\n",
    "        # remove linebreak from a current name\n",
    "        # linebreak is the last character of each line\n",
    "        x = line[:-1]\n",
    "        # add current item to the list\n",
    "        notepad_stops_words.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "notepad_stops_words = ['lol', 'a', 'kk']\n",
    "notepad_stops_words.extend(custom_stops_words)\n",
    "print(notepad_stops_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4467bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data['notepad_stop_word'] = text_data['lemma'].apply(stopwords_remover,stopW=custom_stops_words)\n",
    "text_data[['Text','lemma', 'notepad_stop_word']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f32145",
   "metadata": {},
   "source": [
    "\n",
    "### Apply word2vec or fasttext on same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4dbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec - Similar words to 'میں': [('یہ', 0.5641902685165405), ('وہ', 0.5513450503349304)]\n",
      "FastText - Similar words to 'میں': [('میٹ۔میں', 0.7997676134109497), ('میںڑک', 0.7920107245445251)]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Extract text data from your DataFrame and tokenize it (assuming you have a 'Text' column in your DataFrame)\n",
    "tokenized_data = [text.split() for text in text_data['Text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_data, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Example: Finding similar words using Word2Vec\n",
    "similar_words_word2vec = word2vec_model.wv.most_similar(\"میں\", topn=2)\n",
    "print(\"Word2Vec - Similar words to 'میں':\", similar_words_word2vec)\n",
    "\n",
    "# Example: Finding similar words using FastText\n",
    "similar_words_fasttext = fasttext_model.wv.most_similar(\"میں\", topn=2)\n",
    "print(\"FastText - Similar words to 'میں':\", similar_words_fasttext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de68302",
   "metadata": {},
   "source": [
    "### 10. Apply biagram and unigram on text and discuss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0a43b",
   "metadata": {},
   "source": [
    "##### Making token from 'notepad_stop_word' again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444638a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data['notepad_stop_word_tokens'] = text_data[\"notepad_stop_word\"].apply(customWordTokenizer)\n",
    "text_data[['notepad_stop_word','notepad_stop_word_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b39c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_N_grams(text,ngram):\n",
    "    temp=zip(*[text[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef459215",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_value = 1\n",
    "text_data[f'{ngram_value}_gram'] = text_data[\"tokens\"].apply(generate_N_grams, ngram=ngram_value)\n",
    "text_data[['Text','tokens',f'{ngram_value}_gram']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_value = 2\n",
    "text_data[f'{ngram_value}_gram'] = text_data[\"tokens\"].apply(generate_N_grams, ngram=ngram_value)\n",
    "text_data[['Text','tokens',f'{ngram_value}_gram']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f788ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_textValues=defaultdict(int)\n",
    "for text in text_data['notepad_stop_word_tokens'].to_list():\n",
    "    for word in generate_N_grams(text, ngram=1):\n",
    "        unigram_textValues[word]+=1\n",
    "df_unigram=pd.DataFrame(sorted(unigram_textValues.items(),key=lambda x:x[1],reverse=True))\n",
    "nud1=df_unigram[0][:20]\n",
    "nud2=df_unigram[1][:20]\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='green', width = 0.4)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 20 words in dataframe-UNIGRAM ANALYSIS without syopwords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "nud1=df_unigram[0][-20:]\n",
    "nud2=df_unigram[1][-20:]\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='green', width = 0.4)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Least 20 words in dataframe-UNIGRAM ANALYSIS without syopwords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3fa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the count of every word in both the columns of df_train and df_test dataframes where sentiment=\"neutral\"\n",
    "unigram_textValues=defaultdict(int)\n",
    "for text in text_data['tokens'].to_list():\n",
    "    for word in generate_N_grams(text, ngram=1):\n",
    "        unigram_textValues[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus on more frequently occuring words for every sentiment=>\n",
    "#sort in DO wrt 2nd column in each of positiveValues,negativeValues and neutralValues\n",
    "df_unigram=pd.DataFrame(sorted(unigram_textValues.items(),key=lambda x:x[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e51b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "nud1=df_unigram[0][:20]\n",
    "nud2=df_unigram[1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f446f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='green', width = 0.4)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 20 words in dataframe-UNIGRAM ANALYSIS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_textValues=defaultdict(int)\n",
    "for text in text_data['tokens'].to_list():\n",
    "    for word in generate_N_grams(text, ngram=2):\n",
    "        bigram_textValues[word]+=1\n",
    "df_bigram=pd.DataFrame(sorted(bigram_textValues.items(),key=lambda x:x[1],reverse=True))\n",
    "nud1=df_bigram[0][:20]\n",
    "nud2=df_bigram[1][:20]\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='blue', width = 0.4)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Top 20 words in dataframe-BI-GRAM ANALYSIS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963efd98",
   "metadata": {},
   "source": [
    "### 11. Identify most frequently and least frequently words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neutral=pd.DataFrame(sorted(unigram_textValues.items(),key=lambda x:x[1]))\n",
    "nud1=df_neutral[0][:15]\n",
    "nud2=df_neutral[1][:15]\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='blue', width = 0.5)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Least 15 frequent words in data\")\n",
    "plt.legend(['Most frequent words'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af06e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unigram==pd.DataFrame(sorted(unigram_textValues.items(),key=lambda x:x[1],reverse=True))\n",
    "nud1=df_unigram[0][:20]\n",
    "nud2=df_unigram[1][:20]\n",
    "plt.figure(1,figsize=(16,4))\n",
    "plt.bar(nud1, nud2, color ='blue', width = 0.5)\n",
    "plt.xlabel(\"Words in dataframe\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Most 20 frequent words in data\")\n",
    "plt.legend(['Least frequent words'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5103e5",
   "metadata": {},
   "source": [
    "### 12. Do apply tfidf and compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list =text_data['notepad_stop_word'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text_list)\n",
    "print(\"All Unique words\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectors\")\n",
    "print(vectorizer.transform(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    "# this steps generates word counts for the words of corpus\n",
    "word_count_vector=cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46773848",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d0c9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315511c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(),columns=[\"idf_weights\"]) \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5a8bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix \n",
    "count_vector=cv.transform(text_list) \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35723a",
   "metadata": {},
   "source": [
    "##### Checking on First sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4225990",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names_out() \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cb3e0",
   "metadata": {},
   "source": [
    "### 13. Make a word cloud graph from tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d28fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_feature_names = cv.get_feature_names_out() \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=tf_idf_feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tf_idf_feature_names])\n",
    "wordcloud = WordCloud(width=900, height=900, random_state=21, max_font_size=120).generate(all_words) \n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef65a9",
   "metadata": {},
   "source": [
    "### 14. Convert text into vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d146bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names_out() \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "#print the scores \n",
    "vectors = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"vectors\"]) \n",
    "vectors.sort_values(by=[\"vectors\"],ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79afa786",
   "metadata": {},
   "source": [
    "### 15. Identify bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04920660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extraction(sentence):    \n",
    "  ignore = ['a', \"the\", \"is\"]    \n",
    "  words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
    "  cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
    "  return cleaned_text\n",
    "def tokenize(sentences):    \n",
    "  words = []    \n",
    "  for sentence in sentences:        \n",
    "    w = word_extraction(sentence)        \n",
    "    words.extend(w)            \n",
    "    words = sorted(list(set(words)))    \n",
    "  return words\n",
    "def generate_bow(allsentences):        \n",
    "  vocab = tokenize(allsentences)    \n",
    "  print(\"Word List for Document \\n{0} \\n\".format(vocab));\n",
    "  for sentence in allsentences:        \n",
    "    words = word_extraction(sentence)       \n",
    "    bag_vector = numpy.zeros(len(vocab))        \n",
    "    for w in words:            \n",
    "      for i,word in enumerate(vocab):                \n",
    "        if word == w:                     \n",
    "          bag_vector[i] += 1                            \n",
    "          print(\"{0}\\n{1}\\n\".format(sentence,numpy.array(bag_vector)))\n",
    "generate_bow(text_data[\"Text\"].tolist()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa167c3",
   "metadata": {},
   "source": [
    "### 16. Try to assign label manually / automatically on the basis of index term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label_of_dataset(df,common_words=[\"data\",\"network\",\"algo\",\"algorithm\",\"system\",\"design\",\"web\",\"business\",\"marketing\"] ,column =\"Text\"):\n",
    "  classes= {\"computer\":[\"data\",\"network\",\"algo\",\"algorithm\",\"system\",\"design\",\"web\"],\"Business\":[\"business\",\"marketing\",\"advertisements\"]}\n",
    "  labels =  []\n",
    "  def get_key(val,my_dict):\n",
    "    for key, value in my_dict.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "  for sent in df[column].tolist():\n",
    "    sent = sent.lower()\n",
    "    words = sent.split(\" \")\n",
    "    max = 0\n",
    "    for pos,c_word in enumerate(common_words):\n",
    "      if words.count(c_word)>max:\n",
    "        max = words.count(c_word)\n",
    "        n_word = c_word\n",
    "      else:\n",
    "        if (pos==len(common_words)-1) and (max == 0):\n",
    "          n_word = \"Unidentified\"\n",
    "    for pos2,class1 in enumerate(classes.values()): \n",
    "      if n_word in class1:\n",
    "        labels.append(get_key(class1,classes))\n",
    "        break\n",
    "      else:\n",
    "        if pos2 ==len(classes)-1:\n",
    "          labels.append(n_word)\n",
    "          continue\n",
    "  df[\"Labels\"] = labels\n",
    "  return df\n",
    "new_df = make_label_of_dataset(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d584e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[new_df[\"Labels\"]==\"Business\",[\"Text\",\"Labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511063e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.loc[new_df[\"Labels\"]==\"computer\",[\"Text\",\"Labels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986971c",
   "metadata": {},
   "source": [
    "### 17. Draw some more graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0fed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_KDE(df):\n",
    "  df['word_count'] = df['Text'].apply(lambda x : len(str(x).split(\" \")))\n",
    "  plt.figure(figsize=(12,6))\n",
    "  computer = new_df.loc[new_df[\"Labels\"]==\"computer\",[\"Text\",'word_count',\"Labels\"]]\n",
    "  busineess = new_df.loc[new_df[\"Labels\"]==\"Business\",[\"Text\",'word_count',\"Labels\"]]\n",
    "  unid = new_df.loc[new_df[\"Labels\"]==\"Unidentified\",[\"Text\",'word_count',\"Labels\"]]\n",
    "  p1=sns.kdeplot(computer['word_count'], shade=True, color=\"g\").set_title('Kernel Distribution of Number Of words')\n",
    "  p1=sns.kdeplot(unid['word_count'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\n",
    "  p1=sns.kdeplot(busineess['word_count'], shade=True, color=\"b\")\n",
    "  plt.grid()\n",
    "make_KDE(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "computer = new_df.loc[new_df[\"Labels\"]==\"computer\",['word_count']]\n",
    "busineess = new_df.loc[new_df[\"Labels\"]==\"Business\",['word_count']]\n",
    "unid = new_df.loc[new_df[\"Labels\"]==\"Unidentified\",['word_count']]\n",
    "hist_data = [computer[\"word_count\"] ,busineess[\"word_count\"] ,unid[\"word_count\"] ]\n",
    "group_labels = [\"Computer\",\"Business\",\"Unidentified\"]\n",
    "\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels,show_curve=False)\n",
    "fig.update_layout(title_text='Distribution of Number Of words')\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    "    paper_bgcolor=\"LightSteelBlue\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bf85a",
   "metadata": {},
   "source": [
    "### 18. Part of speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tagged = nltk.pos_tag(tf_idf_feature_names) \n",
    "for  v in tagged:\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bde90b",
   "metadata": {},
   "source": [
    "### 19. Wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc89199",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synsets('web', pos=wn.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea410b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('web.v.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synset('web.v.01').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e07384",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('web.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[str(lemma.name()) for lemma in wn.synset('web.v.01').lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7206329",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in wn.lemmas('think', 'v'):\n",
    "    print(lemma, lemma.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(wn.langs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nltk.FreqDist(\" \".join(text_data['notepad_stop_word'].to_list()).split())\n",
    "d = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())}) \n",
    "d = d.nlargest(columns=\"Count\", n = 20)\n",
    "d.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in d['Hashtag'].to_list():\n",
    "#     wn.synsets(i)\n",
    "    synset_array = wn.synsets(i)\n",
    "#     print()\n",
    "    print_list = []\n",
    "    for j in range(len(synset_array)):\n",
    "        print_list.extend(synset_array[j].lemma_names())\n",
    "#         try:\n",
    "    print(f'{i} -> {list(set(print_list))}')\n",
    "#             print(f'{i} -> {synset_array[1].lemma_names()}')\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(wn.synset('dog.n.01').lemmas('dan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in wn.synset('think.v.01').lemmas():\n",
    "    print(lemma, lemma.frame_ids())\n",
    "    print(\" | \".join(lemma.frame_strings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"web\"):\n",
    "    for i in syn.lemmas():\n",
    "        synonyms.append(i.name())\n",
    "\n",
    "print(set(synonyms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
